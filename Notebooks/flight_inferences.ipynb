{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers = 2, threads_per_worker = 1, memory_limit = \"4GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns s3 client\n",
    "def get_s3_client():\n",
    "    return boto3.client('s3')\n",
    "\n",
    "#lists all objects in an s3 bucket\n",
    "def list_s3_objects(bucket_name, prefix=None):\n",
    "    s3 = get_s3_client()\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return response.get('Contents', [])\n",
    "\n",
    "#downloads files from s3 bucket \n",
    "def download_s3_object(bucket_name, object_key, download_path):\n",
    "    s3 = get_s3_client()\n",
    "    s3.download_file(bucket_name, object_key, download_path)\n",
    "    print(f\"Downloaded {object_key} to {download_path}\")\n",
    "\n",
    "#reads content of file from s3 bucket into memory\n",
    "def read_s3_object(bucket_name, object_key):\n",
    "    s3 = get_s3_client()\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    return response['Body'].read()\n",
    "\n",
    "\n",
    "def list_s3_objects(bucket_name, prefix=\"\"):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return response.get('Contents', [])\n",
    "\n",
    "bucket_name = \"historicalflightdata\"\n",
    "folder_name = \"raw_hist_flight_data/\"\n",
    "\n",
    "s3_client = boto3.client('s3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'ARR_TIME': 'float64',\n",
    "    'CANCELLATION_CODE': 'object',  # Keep this as 'object' since it contains mixed types\n",
    "    'DEP_TIME': 'float64'\n",
    "}\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"historicalflightdata\"\n",
    "folder_name = \"raw_hist_flight_data/\"  # Specify your folder here, make sure it's a valid string\n",
    "\n",
    "# Function to list objects in S3 bucket\n",
    "def list_s3_objects(bucket_name, prefix=\"\"):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    return response.get('Contents', [])\n",
    "\n",
    "# List the files in the folder\n",
    "response = list_s3_objects(bucket_name, folder_name)\n",
    "\n",
    "# List to store the file paths\n",
    "file_keys = [obj.get('Key') for obj in response if obj.get('Key').endswith('.csv')]\n",
    "\n",
    "# Define the S3 URL path\n",
    "s3_paths = [f\"s3://{bucket_name}/{file_key}\" for file_key in file_keys]\n",
    "\n",
    "# Use Dask to read the parquet files from S3 in parallel\n",
    "dfs = [dd.read_csv(path , dtype=dtypes) for path in s3_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_dask = dd.concat(dfs)\n",
    "combined_df_dask = combined_df_dask.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of delay columns\n",
    "delay_cols = [\"DEP_DELAY\", \"ARR_DELAY\", \"CARRIER_DELAY\", \"WEATHER_DELAY\", \n",
    "              \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\"]\n",
    "\n",
    "combined_df_dask[delay_cols] = combined_df_dask[delay_cols].fillna(0) #replace missing values with 0\n",
    "combined_df_dask[\"Delay\"] = combined_df_dask[delay_cols].sum(axis=1) #sums all delay columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up FL_DATA column\n",
    "\n",
    "combined_df_dask[\"FL_DATE\"] = dd.to_datetime(combined_df_dask[\"FL_DATE\"], format=\"%m/%d/%Y %I:%M:%S %p\", errors=\"coerce\")\n",
    "combined_df_dask[\"FL_DATE\"] = combined_df_dask[\"FL_DATE\"].dt.date #removes the timestamp\n",
    "combined_df_dask[\"FL_DATE\"] = dd.to_datetime(combined_df_dask[\"FL_DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 17:\n",
    "        return \"afternoon\"\n",
    "    elif 17 <= hour < 21:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "    \n",
    "#applied to each partition of the dask df\n",
    "def process_partition(partition):\n",
    "    partition = partition.copy()\n",
    "    partition['hour'] = partition['DEP_TIME'].fillna(0).astype(int) // 100\n",
    "    partition['Time_of_Day'] = partition['hour'].apply(assign_time_of_day)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_dask = combined_df_dask.map_partitions(\n",
    "    process_partition,\n",
    "    meta=combined_df_dask.assign(hour=0, Time_of_Day='object')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning seasons depending on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9,10,11]:\n",
    "        return \"Fall\"\n",
    "    else:\n",
    "        return \"NA\" #is this fine?\n",
    "    \n",
    "\n",
    "#creates column 'month' and 'Season' which is applied to a partition\n",
    "def season_process_partition(partition):\n",
    "    partition = partition.copy()\n",
    "    partition['month'] = partition['FL_DATE'].dt.month\n",
    "    partition['Season'] = partition['month'].apply(get_season)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_dask = combined_df_dask.map_partitions(\n",
    "    season_process_partition,\n",
    "    meta=combined_df_dask.assign(month=0, Season ='object')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Route_Pair column based on ORIGIN + DEST airports\n",
    "combined_df_dask[\"Route_Pair\"] = combined_df_dask[\"ORIGIN\"] + \" to \" + combined_df_dask[\"DEST\"]\n",
    "\n",
    "#grouping and performing aggregations\n",
    "grouped_combined_df = combined_df_dask.groupby([\"Route_Pair\", \"Time_of_Day\", \"OP_CARRIER\", \"Season\"])[\"Delay\"].agg([\"mean\", \"std\", \"count\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"2nd half\" of CI formula \n",
    "grouped_combined_df[\"CI_2nd_Half\"] = 1.96 * (grouped_combined_df[\"std\"] /np.sqrt(grouped_combined_df[\"count\"]))\n",
    "\n",
    "#lower and upper bounds of CI \n",
    "grouped_combined_df[\"CI_Lower\"] = grouped_combined_df[\"mean\"] - grouped_combined_df[\"CI_2nd_Half\"]\n",
    "grouped_combined_df[\"CI_Upper\"] = grouped_combined_df[\"mean\"] + grouped_combined_df[\"CI_2nd_Half\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_combined_df = grouped_combined_df.repartition(npartitions= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "# saves the the Dask DataFrame as a Parquet file in S3 bucket - no longer needed since can compute in cell below?\n",
    "\n",
    "# grouped_combined_df.to_parquet(\n",
    "#     \"s3://historicalflightdata/inference_outputs/\",\n",
    "#     engine=\"pyarrow\",\n",
    "#     write_index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs .compute() then saves to Parquet file in S3 bucket\n",
    "\n",
    "grouped_combined_df.compute().to_parquet(\n",
    "    \"s3://historicalflightdata/inference_outputs/output.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
